{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13052178,"sourceType":"datasetVersion","datasetId":8207902}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:52.713715Z","iopub.execute_input":"2025-09-14T09:48:52.716162Z","iopub.status.idle":"2025-09-14T09:48:52.730100Z","shell.execute_reply.started":"2025-09-14T09:48:52.716125Z","shell.execute_reply":"2025-09-14T09:48:52.724932Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sri-lankan-youtube-video-data/FInalProcessedDataset1.csv\n/kaggle/input/sri-lankan-youtube-video-data/yt_data_merged_v2.csv\n/kaggle/input/sri-lankan-youtube-video-data/FinalProcessedDataset_03.csv\n/kaggle/input/sri-lankan-youtube-video-data/final_dataset_with_trending_unique_vids3.csv\n/kaggle/input/sri-lankan-youtube-video-data/yt_data_merged.csv\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sri-lankan-youtube-video-data/FinalProcessedDataset_03.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:52.732562Z","iopub.execute_input":"2025-09-14T09:48:52.732836Z","iopub.status.idle":"2025-09-14T09:48:55.183514Z","shell.execute_reply.started":"2025-09-14T09:48:52.732804Z","shell.execute_reply":"2025-09-14T09:48:55.177947Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.185782Z","iopub.execute_input":"2025-09-14T09:48:55.186063Z","iopub.status.idle":"2025-09-14T09:48:55.223639Z","shell.execute_reply.started":"2025-09-14T09:48:55.186039Z","shell.execute_reply":"2025-09-14T09:48:55.219623Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                               title    channel_name  \\\n0  Watch full short film!!üëÜ@kaidoleeroberts429 #s...  Sofi Manassyan   \n1      ToRung short film: üôè let's love each other ‚ù§Ô∏è          ToRung   \n2  He messed with a wrong person #movieexplainedi...     CineClarity   \n3  ‡¶∏‡¶¨ ‡¶ï‡ßÅ‡¶≤‡¶ø ‡¶ö‡ßã‡¶∞ ‡¶®‡¶æ‡•§ short film _natok3.2_bsngla na...        Natok3.2   \n4  üò±Movie Recap Short Film Mystery Explained in H...         HowTo1C   \n\n      video_id                channel_id               publish_date  \\\n0  o8rP66qVN-4  UCZFBnnCCO65xMXOdtFz8CfA  2025-09-06 18:14:11+00:00   \n1  07TTa5FHqz8  UCXbYlU08sOTBktOtjVsvR6w  2025-09-06 00:41:33+00:00   \n2  6VfcnEBZ2dg  UCOHMJ8XnZL5TKZbY7Pj1yLA  2025-09-07 03:16:44+00:00   \n3  BwR2GU0kovc  UCKkv7Lm0fprBm5LSaaKlZoA  2025-09-06 17:26:54+00:00   \n4  EYBCNN_9E2M  UCtkFE68zgVgQimnkgJBSVmA  2025-09-07 09:30:08+00:00   \n\n               snapshot_date country  view_count  like_count  comment_count  \\\n0  2025-09-11 00:00:00+00:00      LK      286283       12778            277   \n1  2025-09-11 00:00:00+00:00      LK      537527        6174             12   \n2  2025-09-11 00:00:00+00:00      LK     2693713           0            285   \n3  2025-09-11 00:00:00+00:00      LK       72606         723              1   \n4  2025-09-11 00:00:00+00:00      LK       22894           0              1   \n\n   ... is_holiday   publish_date_naive title_len desc_len tag_count  \\\n0  ...      False  2025-09-06 00:00:00        77      0.0         1   \n1  ...      False  2025-09-06 00:00:00        45    139.0        13   \n2  ...       True  2025-09-07 00:00:00        60    396.0        17   \n3  ...      False  2025-09-06 00:00:00        50   1591.0         1   \n4  ...       True  2025-09-07 00:00:00        98   2709.0         1   \n\n   has_hashtag views_per_hour_norm  duration_timedelta  duration_minutes  \\\n0            1            0.009381     0 days 00:01:48          1.800000   \n1            0            0.017613     0 days 00:00:37          0.616667   \n2            1            0.088266     0 days 00:02:58          2.966667   \n3            0            0.002379     0 days 00:03:01          3.016667   \n4            1            0.000750     0 days 00:00:37          0.616667   \n\n      category_name  \n0  Film & Animation  \n1            Comedy  \n2     Entertainment  \n3     Entertainment  \n4     Entertainment  \n\n[5 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>channel_name</th>\n      <th>video_id</th>\n      <th>channel_id</th>\n      <th>publish_date</th>\n      <th>snapshot_date</th>\n      <th>country</th>\n      <th>view_count</th>\n      <th>like_count</th>\n      <th>comment_count</th>\n      <th>...</th>\n      <th>is_holiday</th>\n      <th>publish_date_naive</th>\n      <th>title_len</th>\n      <th>desc_len</th>\n      <th>tag_count</th>\n      <th>has_hashtag</th>\n      <th>views_per_hour_norm</th>\n      <th>duration_timedelta</th>\n      <th>duration_minutes</th>\n      <th>category_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Watch full short film!!üëÜ@kaidoleeroberts429 #s...</td>\n      <td>Sofi Manassyan</td>\n      <td>o8rP66qVN-4</td>\n      <td>UCZFBnnCCO65xMXOdtFz8CfA</td>\n      <td>2025-09-06 18:14:11+00:00</td>\n      <td>2025-09-11 00:00:00+00:00</td>\n      <td>LK</td>\n      <td>286283</td>\n      <td>12778</td>\n      <td>277</td>\n      <td>...</td>\n      <td>False</td>\n      <td>2025-09-06 00:00:00</td>\n      <td>77</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.009381</td>\n      <td>0 days 00:01:48</td>\n      <td>1.800000</td>\n      <td>Film &amp; Animation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ToRung short film: üôè let's love each other ‚ù§Ô∏è</td>\n      <td>ToRung</td>\n      <td>07TTa5FHqz8</td>\n      <td>UCXbYlU08sOTBktOtjVsvR6w</td>\n      <td>2025-09-06 00:41:33+00:00</td>\n      <td>2025-09-11 00:00:00+00:00</td>\n      <td>LK</td>\n      <td>537527</td>\n      <td>6174</td>\n      <td>12</td>\n      <td>...</td>\n      <td>False</td>\n      <td>2025-09-06 00:00:00</td>\n      <td>45</td>\n      <td>139.0</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0.017613</td>\n      <td>0 days 00:00:37</td>\n      <td>0.616667</td>\n      <td>Comedy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>He messed with a wrong person #movieexplainedi...</td>\n      <td>CineClarity</td>\n      <td>6VfcnEBZ2dg</td>\n      <td>UCOHMJ8XnZL5TKZbY7Pj1yLA</td>\n      <td>2025-09-07 03:16:44+00:00</td>\n      <td>2025-09-11 00:00:00+00:00</td>\n      <td>LK</td>\n      <td>2693713</td>\n      <td>0</td>\n      <td>285</td>\n      <td>...</td>\n      <td>True</td>\n      <td>2025-09-07 00:00:00</td>\n      <td>60</td>\n      <td>396.0</td>\n      <td>17</td>\n      <td>1</td>\n      <td>0.088266</td>\n      <td>0 days 00:02:58</td>\n      <td>2.966667</td>\n      <td>Entertainment</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>‡¶∏‡¶¨ ‡¶ï‡ßÅ‡¶≤‡¶ø ‡¶ö‡ßã‡¶∞ ‡¶®‡¶æ‡•§ short film _natok3.2_bsngla na...</td>\n      <td>Natok3.2</td>\n      <td>BwR2GU0kovc</td>\n      <td>UCKkv7Lm0fprBm5LSaaKlZoA</td>\n      <td>2025-09-06 17:26:54+00:00</td>\n      <td>2025-09-11 00:00:00+00:00</td>\n      <td>LK</td>\n      <td>72606</td>\n      <td>723</td>\n      <td>1</td>\n      <td>...</td>\n      <td>False</td>\n      <td>2025-09-06 00:00:00</td>\n      <td>50</td>\n      <td>1591.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.002379</td>\n      <td>0 days 00:03:01</td>\n      <td>3.016667</td>\n      <td>Entertainment</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>üò±Movie Recap Short Film Mystery Explained in H...</td>\n      <td>HowTo1C</td>\n      <td>EYBCNN_9E2M</td>\n      <td>UCtkFE68zgVgQimnkgJBSVmA</td>\n      <td>2025-09-07 09:30:08+00:00</td>\n      <td>2025-09-11 00:00:00+00:00</td>\n      <td>LK</td>\n      <td>22894</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>True</td>\n      <td>2025-09-07 00:00:00</td>\n      <td>98</td>\n      <td>2709.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.000750</td>\n      <td>0 days 00:00:37</td>\n      <td>0.616667</td>\n      <td>Entertainment</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 34 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.224456Z","iopub.execute_input":"2025-09-14T09:48:55.226179Z","iopub.status.idle":"2025-09-14T09:48:55.329730Z","shell.execute_reply.started":"2025-09-14T09:48:55.226155Z","shell.execute_reply":"2025-09-14T09:48:55.325350Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 97766 entries, 0 to 97765\nData columns (total 34 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   title                97766 non-null  object \n 1   channel_name         97766 non-null  object \n 2   video_id             97766 non-null  object \n 3   channel_id           97766 non-null  object \n 4   publish_date         97766 non-null  object \n 5   snapshot_date        97766 non-null  object \n 6   country              97766 non-null  object \n 7   view_count           97766 non-null  int64  \n 8   like_count           97766 non-null  int64  \n 9   comment_count        97766 non-null  int64  \n 10  description          97766 non-null  object \n 11  thumbnail_url        97766 non-null  object \n 12  video_tags           97766 non-null  object \n 13  kind                 97766 non-null  object \n 14  language             97766 non-null  object \n 15  category_id          97766 non-null  float64\n 16  duration             97766 non-null  object \n 17  days_since_publish   97766 non-null  float64\n 18  publish_weekday      97766 non-null  int64  \n 19  publish_hour         97766 non-null  int64  \n 20  is_weekend           97766 non-null  int64  \n 21  slot_id              97766 non-null  int64  \n 22  part_of_day          97766 non-null  object \n 23  views_per_hour       97766 non-null  float64\n 24  is_holiday           97766 non-null  bool   \n 25  publish_date_naive   97766 non-null  object \n 26  title_len            97766 non-null  int64  \n 27  desc_len             97766 non-null  float64\n 28  tag_count            97766 non-null  int64  \n 29  has_hashtag          97766 non-null  int64  \n 30  views_per_hour_norm  97766 non-null  float64\n 31  duration_timedelta   97766 non-null  object \n 32  duration_minutes     97766 non-null  float64\n 33  category_name        97766 non-null  object \ndtypes: bool(1), float64(6), int64(10), object(17)\nmemory usage: 24.7+ MB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.330425Z","iopub.execute_input":"2025-09-14T09:48:55.330625Z","iopub.status.idle":"2025-09-14T09:48:55.436661Z","shell.execute_reply.started":"2025-09-14T09:48:55.330605Z","shell.execute_reply":"2025-09-14T09:48:55.430806Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"title                  0\nchannel_name           0\nvideo_id               0\nchannel_id             0\npublish_date           0\nsnapshot_date          0\ncountry                0\nview_count             0\nlike_count             0\ncomment_count          0\ndescription            0\nthumbnail_url          0\nvideo_tags             0\nkind                   0\nlanguage               0\ncategory_id            0\nduration               0\ndays_since_publish     0\npublish_weekday        0\npublish_hour           0\nis_weekend             0\nslot_id                0\npart_of_day            0\nviews_per_hour         0\nis_holiday             0\npublish_date_naive     0\ntitle_len              0\ndesc_len               0\ntag_count              0\nhas_hashtag            0\nviews_per_hour_norm    0\nduration_timedelta     0\nduration_minutes       0\ncategory_name          0\ndtype: int64"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"numerical_features = ['view_count', 'like_count', 'comment_count', 'publish_hour', 'days_since_publish', \n                     'is_weekend', 'views_per_hour', 'duration_minutes', 'title_len', 'desc_len', 'tag_count']\ncategorical_features = ['country', 'kind', 'language', 'category_id', 'part_of_day', 'is_holiday']\n\n# Handle missing values\ndf[numerical_features] = df[numerical_features].fillna(0)\ndf[categorical_features] = df[categorical_features].fillna('unknown')\n\n# Convert boolean to int (0/1) for is_weekend and is_holiday\ndf['is_weekend'] = df['is_weekend'].astype(int)\ndf['is_holiday'] = df['is_holiday'].astype(int)\n\n# Create vocabulary mappings with padding index for unknown\nvocab_maps = {}\nfor cat in categorical_features:\n    unique_vals = df[cat].unique()\n    vocab_maps[cat] = {'unknown': 0}  # Padding/unknown index\n    vocab_maps[cat].update({val: idx + 1 for idx, val in enumerate(unique_vals[unique_vals != 'unknown'])})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.438166Z","iopub.execute_input":"2025-09-14T09:48:55.438394Z","iopub.status.idle":"2025-09-14T09:48:55.510953Z","shell.execute_reply.started":"2025-09-14T09:48:55.438373Z","shell.execute_reply":"2025-09-14T09:48:55.506807Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Normalize numerical features with min-max scaling\nscaler = StandardScaler()\ndf[numerical_features] = scaler.fit_transform(df[numerical_features])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.513309Z","iopub.execute_input":"2025-09-14T09:48:55.513528Z","iopub.status.idle":"2025-09-14T09:48:55.544483Z","shell.execute_reply.started":"2025-09-14T09:48:55.513507Z","shell.execute_reply":"2025-09-14T09:48:55.540046Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Split into train/test (for inference on different subsets)\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.547097Z","iopub.execute_input":"2025-09-14T09:48:55.547315Z","iopub.status.idle":"2025-09-14T09:48:55.678392Z","shell.execute_reply.started":"2025-09-14T09:48:55.547294Z","shell.execute_reply":"2025-09-14T09:48:55.672951Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Custom Dataset class for PyTorch\nclass MetadataDataset(Dataset):\n    def __init__(self, df, numerical_features, categorical_features, vocab_maps):\n        self.numerical = torch.tensor(df[numerical_features].values, dtype=torch.float32)\n        self.categorical = {\n            cat: torch.tensor([vocab_maps[cat][val] for val in df[cat]], dtype=torch.long)\n            for cat in categorical_features\n        }\n\n    def __len__(self):\n        return len(self.numerical)\n\n    def __getitem__(self, idx):\n        return {\n            'numerical': self.numerical[idx],\n            **{cat: self.categorical[cat][idx] for cat in self.categorical}\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.680581Z","iopub.execute_input":"2025-09-14T09:48:55.681178Z","iopub.status.idle":"2025-09-14T09:48:55.692856Z","shell.execute_reply.started":"2025-09-14T09:48:55.681152Z","shell.execute_reply":"2025-09-14T09:48:55.688213Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = MetadataDataset(train_df, numerical_features, categorical_features, vocab_maps)\ntest_dataset = MetadataDataset(test_df, numerical_features, categorical_features, vocab_maps)\n\n# DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.693483Z","iopub.execute_input":"2025-09-14T09:48:55.693686Z","iopub.status.idle":"2025-09-14T09:48:55.856295Z","shell.execute_reply.started":"2025-09-14T09:48:55.693661Z","shell.execute_reply":"2025-09-14T09:48:55.851727Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Model Instantiation\n# Embedding dims with increased sizes for semantic capture\nembed_dims = {\n    'country': (len(vocab_maps['country']), 32),\n    'kind': (len(vocab_maps['kind']), 16),\n    'language': (len(vocab_maps['language']), 64),\n    'category_id': (len(vocab_maps['category_id']), 32),\n    'part_of_day': (len(vocab_maps['part_of_day']), 16),\n    'is_holiday': (len(vocab_maps['is_holiday']), 8)\n}\n\nmodel = MetadataTower(\n    num_numerical=len(numerical_features),\n    embed_dims=embed_dims,\n    ffnn_layers=[256, 256],\n    output_dim=256\n)\n\n# Move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.875339Z","iopub.execute_input":"2025-09-14T09:48:55.875570Z","iopub.status.idle":"2025-09-14T09:48:55.898134Z","shell.execute_reply.started":"2025-09-14T09:48:55.875546Z","shell.execute_reply":"2025-09-14T09:48:55.892978Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"MetadataTower(\n  (embeddings): ModuleDict(\n    (country): Embedding(2, 32, padding_idx=0)\n    (kind): Embedding(2, 16, padding_idx=0)\n    (language): Embedding(65, 64, padding_idx=0)\n    (category_id): Embedding(17, 32, padding_idx=0)\n    (part_of_day): Embedding(5, 16, padding_idx=0)\n    (is_holiday): Embedding(3, 8, padding_idx=0)\n  )\n  (attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n  )\n  (ffnn): Sequential(\n    (0): Linear(in_features=179, out_features=256, bias=True)\n    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    (2): ReLU()\n    (3): Linear(in_features=256, out_features=256, bias=True)\n    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    (5): ReLU()\n    (6): Linear(in_features=256, out_features=256, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# MetadataTower Module\nclass MetadataTower(nn.Module):\n    def __init__(self, num_numerical, embed_dims, ffnn_layers=[128, 256], output_dim=256):\n        super(MetadataTower, self).__init__()\n        \n        # Categorical Embeddings with larger dims for semantic richness\n        self.embeddings = nn.ModuleDict({\n            cat: nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n            for cat, (vocab_size, embed_dim) in embed_dims.items()\n        })\n        \n        # Attention mechanism for feature weighting\n        self.attention = nn.MultiheadAttention(embed_dim=output_dim, num_heads=4)\n        \n        # Calculate input dim for FFNN: numerical + sum of embed dims\n        embed_total_dim = sum(embed_dim for _, embed_dim in embed_dims.values())\n        input_dim = num_numerical + embed_total_dim\n        \n        # Feedforward Neural Network (MLP) with attention integration\n        layers = []\n        current_dim = input_dim\n        for hidden_dim in ffnn_layers:\n            layers.append(nn.Linear(current_dim, hidden_dim))\n            layers.append(nn.LayerNorm(hidden_dim))  # Add normalization for stability\n            layers.append(nn.ReLU())\n            current_dim = hidden_dim\n        layers.append(nn.Linear(current_dim, output_dim))\n        self.ffnn = nn.Sequential(*layers)\n\n    def forward(self, numerical, categorical):\n        # Embed categorical features\n        embed_outs = [self.embeddings[cat](categorical[cat]) for cat in categorical_features]\n        \n        # Concatenate embeddings and numerical features\n        combined = torch.cat([numerical] + embed_outs, dim=1)  # Shape: [batch, input_dim]\n        \n        # Pass through FFNN to get initial embedding\n        ffnn_out = self.ffnn(combined)  # Shape: [batch, output_dim]\n        \n        # Apply self-attention to capture semantic relationships\n        attn_out, _ = self.attention(ffnn_out.unsqueeze(1), ffnn_out.unsqueeze(1), ffnn_out.unsqueeze(1))\n        attn_out = attn_out.squeeze(1)  # Shape: [batch, output_dim]\n        \n        # Final metadata vector with semantic essence\n        metadata_vec = nn.functional.normalize(attn_out, p=2, dim=1)  # L2 normalization\n        return metadata_vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.858408Z","iopub.execute_input":"2025-09-14T09:48:55.858653Z","iopub.status.idle":"2025-09-14T09:48:55.873385Z","shell.execute_reply.started":"2025-09-14T09:48:55.858630Z","shell.execute_reply":"2025-09-14T09:48:55.868992Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Generate Embeddings\nmodel.eval()\nall_embeddings = []\nwith torch.no_grad():\n    for batch in test_loader:\n        numerical = batch['numerical'].to(device)\n        categorical = {cat: batch[cat].to(device) for cat in categorical_features}\n        metadata_vec = model(numerical, categorical)\n        all_embeddings.append(metadata_vec.cpu().numpy())\n\n# Concatenate all batch embeddings\nall_embeddings = np.concatenate(all_embeddings, axis=0)\nprint(\"Total Embeddings Shape:\", all_embeddings.shape)  # Should be [num_samples, 256]\nprint(\"Sample Embedding (first 5 dims of first sample):\", all_embeddings[0][:5])\n\nprint(\"Metadata Tower built with semantic embeddings!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T09:48:55.899987Z","iopub.execute_input":"2025-09-14T09:48:55.900227Z","iopub.status.idle":"2025-09-14T09:48:57.512961Z","shell.execute_reply.started":"2025-09-14T09:48:55.900205Z","shell.execute_reply":"2025-09-14T09:48:57.506100Z"}},"outputs":[{"name":"stdout","text":"Total Embeddings Shape: (19554, 256)\nSample Embedding (first 5 dims of first sample): [ 0.0539716  -0.02611773 -0.1037618   0.16397811  0.06283554]\nMetadata Tower built with semantic embeddings (no target)!\n","output_type":"stream"}],"execution_count":19}]}